{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f205b728-8eaf-4fc4-8238-8b3cd279e50d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<div style=\"text-align:center\">\n",
    "  \n",
    "# **Trabalho Final Big Data Analytics**  \n",
    "##**Matheus Nascimento**\n",
    "  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e97343d1-ffbb-4e44-af03-802133ede5c1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Projeto Final**  \n",
    "**BIG DATA ANALYTICS.**  \n",
    "Você foi contratado como consultor em Big Data Analytics pelo ministério de saúde dos Estados Unidos par analisar os mais recentes dados da COVID-19. Os seus 2 grandes objetivos são um procedimento de ingestão, transformação e carregamento dos dados (Extract Transformation and Load); e a outro é a análise dos dados.  \n",
    "\n",
    "\n",
    "**Primeira Parte (ETL)**  \n",
    "1. Como consultor em Analytics, a primeira tarefa é criar um pipeline para carregar aos dados que contêm a informação de cada doente. É o seu objetivo e criar um   procedimento para receber ficheiros CSV e carregá-los diretamente no snowflake. Para atingir dito objetivo o HHS(departamento de saúde dos Estados Unidos), pede que o seu   código seja reutilizável porque o processo que você vai criar será usado para ingerir mais dados posteriormente. Crie uma definição para cada um dos processos.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b065b74d-0d3e-4332-814b-de92e051f75b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "import  pyspark.sql.functions as F\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b46c9d2-b7b5-426d-adfd-cbf4d024b0ec",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class ETL:\n",
    "    def __init__(self, snowflake_params):\n",
    "        self.snowflake_params = snowflake_params\n",
    "        self.spark = SparkSession.builder.appName(\"ETL\").getOrCreate()\n",
    "        self.snowflake_connection = None\n",
    "\n",
    "    def establish_snowflake_connection(self):\n",
    "        try:\n",
    "            self.snowflake_connection = self.spark.read.format(\"snowflake\") \\\n",
    "                .option(\"host\", self.snowflake_params[\"host\"]) \\\n",
    "                .option(\"user\", self.snowflake_params[\"user\"]) \\\n",
    "                .option(\"password\", self.snowflake_params[\"password\"]) \\\n",
    "                .option(\"sfWarehouse\", self.snowflake_params[\"warehouse\"]) \\\n",
    "                .option(\"database\", self.snowflake_params[\"database\"]) \\\n",
    "                .option(\"schema\", self.snowflake_params[\"schema\"]) \\\n",
    "                .option(\"dbtable\", self.snowflake_params[\"dbtable\"]) \\\n",
    "                .load()\n",
    "\n",
    "            print(\"Connection to Snowflake established successfully.\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Error establishing Snowflake connection: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "    def read_data_from_snowflake(self, tables_or_queries):\n",
    "        try:\n",
    "            dfs = []\n",
    "            for item in tables_or_queries:\n",
    "                if \" \" in item:\n",
    "                    df = self.spark.read.format(\"snowflake\") \\\n",
    "                        .option(\"host\", self.snowflake_params[\"host\"]) \\\n",
    "                        .option(\"user\", self.snowflake_params[\"user\"]) \\\n",
    "                        .option(\"password\", self.snowflake_params[\"password\"]) \\\n",
    "                        .option(\"sfWarehouse\", self.snowflake_params[\"warehouse\"]) \\\n",
    "                        .option(\"database\", self.snowflake_params[\"database\"]) \\\n",
    "                        .option(\"schema\", self.snowflake_params.get(\"schema\", None)) \\\n",
    "                        .option(\"dbtable\", f\"({item})\") \\\n",
    "                        .load()\n",
    "                else:\n",
    "                    df = self.spark.read.format(\"snowflake\") \\\n",
    "                        .option(\"host\", self.snowflake_params[\"host\"]) \\\n",
    "                        .option(\"user\", self.snowflake_params[\"user\"]) \\\n",
    "                        .option(\"password\", self.snowflake_params[\"password\"]) \\\n",
    "                        .option(\"sfWarehouse\", self.snowflake_params[\"warehouse\"]) \\\n",
    "                        .option(\"database\", self.snowflake_params[\"database\"]) \\\n",
    "                        .option(\"schema\", self.snowflake_params.get(\"schema\", None)) \\\n",
    "                        .option(\"dbtable\", item) \\\n",
    "                        .load()\n",
    "                dfs.append(df)\n",
    "\n",
    "            print(\"Data read successfully.\")\n",
    "            return dfs\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading data from Snowflake: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def write_data_to_snowflake(self, dfs, target_schema, target_table_names=None):\n",
    "        try:\n",
    "            if target_table_names is None:\n",
    "                raise ValueError(\"Target table names must be specified for writing to Snowflake.\")\n",
    "\n",
    "            if len(target_table_names) != len(dfs):\n",
    "                raise ValueError(\"The number of target table names must be the same as the number of DataFrames.\")\n",
    "\n",
    "            start_time = datetime.now()\n",
    "            results = []\n",
    "\n",
    "            for df, target_table in zip(dfs, target_table_names):\n",
    "                df.write.format(\"snowflake\") \\\n",
    "                    .option(\"host\", self.snowflake_params[\"host\"]) \\\n",
    "                    .option(\"user\", self.snowflake_params[\"user\"]) \\\n",
    "                    .option(\"password\", self.snowflake_params[\"password\"]) \\\n",
    "                    .option(\"sfWarehouse\", self.snowflake_params[\"warehouse\"]) \\\n",
    "                    .option(\"database\", self.snowflake_params[\"database\"]) \\\n",
    "                    .option(\"schema\", target_schema) \\\n",
    "                    .option(\"dbtable\", target_table) \\\n",
    "                    .mode(\"overwrite\").save()\n",
    "\n",
    "                results.append({\n",
    "                    \"Schema\": target_schema,\n",
    "                    \"Table\": target_table,\n",
    "                    \"Number of columns\": len(df.columns),\n",
    "                    \"Column names\": df.columns,\n",
    "                    \"Number of rows\": df.count()\n",
    "                })\n",
    "\n",
    "            end_time = datetime.now()\n",
    "            elapsed_time = (end_time - start_time).total_seconds()\n",
    "\n",
    "            print(\"Data written successfully.\")\n",
    "            return {\"Total elapsed time\": elapsed_time, \"Results\": results}\n",
    "        except Exception as e:\n",
    "            print(f\"Error writing data to Snowflake: {str(e)}\")\n",
    "            return {\"Error\": f\"Failed to write data to Snowflake. {str(e)}\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5d05cb7-1a97-4e95-a07d-299074b0a926",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Procedimento:**  \n",
    "\n",
    "\n",
    "**--Use %run \"Users/matheuslpdnascimento@gmail.com/matheus_nascimento_etl\"**\n",
    "\n",
    "**--Defina os parametros do snowflake em um dicionário como este:**    \n",
    "    \n",
    "\n",
    "snowflake_params = {  \n",
    "    \"host\": \"wsraake-pw92122.snowflakecomputing.com\",  \n",
    "    \"user\": \"MATHEUSLDNASCIMENTO\",  \n",
    "    \"password\": \"MNBmnb1!\",  \n",
    "    \"warehouse\": \"COMPUTE_WH\",  \n",
    "    \"database\": \"TRABALHO_FINAL_BIGDATA\",  \n",
    "    \"dbtable\": \"PATIENTS\"  \n",
    "}\n",
    "\n",
    "\n",
    "**--Crie uma instância da classe.**  \n",
    "EXEMPLO: ETL_Processor = ETL(snowflake_params)\n",
    "\n",
    "\n",
    "**--Estabeleça a conexão. Terá que usar uma tabela específica para isso (defina no snowflake_params).**  \n",
    "Função: establish_snowflake_connection(self):  \n",
    "EXEMPLO: ETL_Processor.establish_snowflake_connection()  \n",
    "\n",
    "\n",
    "**--Use a função para ler as tabelas do snowflake. Pode usar uma tabela, uma lista de tabelas ou uma query.**  \n",
    "Função: read_data_from_snowflake(self, tables_or_queries)    \n",
    "EXEMPLO: tables_or_queries = ['PATIENTS', 'CONDITIONS', 'OBSERVATIONS']  \n",
    "[PATIENTS, CONDITIONS, OBSERVATIONS] = ETL_Processor.read_data_from_snowflake(tables_or_queries)  \n",
    "\n",
    "\n",
    "**--Use a função para levar um dataframe até o snowflake. Especifique o schema e os nomes da(s) novas tabelas.**    \n",
    "Função: write_data_to_snowflake(self, dfs, target_schema, target_table_names=None)   \n",
    "EXEMPLO: ETL_Processor.write_data_to_snowflake([PATIENTS, CONDITIONS, OBSERVATIONS], \"COVID19\", ['N_PATIENTS', 'N_CONDITIONS', '_N_OBSERVATIONS'])"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "matheus_nascimento_etl",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
